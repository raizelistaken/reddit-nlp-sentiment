import sys
# print(sys.executable)


from collections import Counter
import pyLDAvis.gensim_models as gensimvis
from gensim import corpora, models, similarities
from gensim.models.ldamulticore import LdaMulticore
from sklearn.decomposition import LatentDirichletAllocation
import nltk
from nltk.sentiment import SentimentIntensityAnalyzer
from nltk.stem import PorterStemmer
from langdetect import detect, DetectorFactory
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from nltk.probability import FreqDist
from nltk.corpus import stopwords
from sklearn.metrics import confusion_matrix
import seaborn as sns
from sklearn.naive_bayes import MultinomialNB
from sklearn import metrics
from sklearn.model_selection import train_test_split
from matplotlib import cm
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score
from sklearn.naive_bayes import MultinomialNB
from tqdm import tqdm
from wordcloud import WordCloud
import nest_asyncio
import praw
from sklearn.manifold import TSNE
import re

nest_asyncio.apply()

nltk.download("vader_lexicon")
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')


from nltk.tokenize import RegexpTokenizer
tokenizer = RegexpTokenizer(r'[a-zA-Z0-9]+')


reddit = praw.Reddit(
    client_id="",
    client_secret="",
    user_agent="",
)


print(reddit.read_only)


reddit


subreddit = reddit.subreddit("movies")

reviews = []
for submission in subreddit.new(limit=100):  # You can adjust the limit
    # if "review" in submission.title.lower():
    #     reviews.append(submission.selftext)  # selftext is the post body
    # keywords = ["review", "thoughts", "opinion", "watched"]
    # if any(kw in submission.title.lower() for kw in keywords):
    #     reviews.append(submission.selftext)
    if submission.selftext:  # Only collect non-empty posts
        reviews.append(submission.selftext)

print(f"Collected {len(reviews)} reviews.")


# reviews[:5]


subreddit = reddit.subreddit("movies")
reviews = []

inception_keywords = ["inception", "INCEPTION", "inception movie"]

for submission in subreddit.new(limit=200):  # You can increase the limit for better coverage
    title = submission.title.lower()
    body = submission.selftext.lower()
    
    if any(kw in title or kw in body for kw in inception_keywords):
        if submission.selftext.strip():  # Make sure it's not empty
            reviews.append(submission.selftext.strip())

print(f"Collected {len(reviews)} Inception-related reviews/posts.")


subreddit = reddit.subreddit("Inception")


for submission in subreddit.new(limit=500):  # Adjust the limit as needed
    if submission.selftext:
        reviews.append(submission.selftext)

print('Collected: ', len(reviews), 'Inception-related reviews/posts.')


DetectorFactory.seed = 0  # To make detection more consistent

reviews = []

for submission in subreddit.new(limit=500):
    if submission.selftext:
        try:
            if detect(submission.selftext) == 'en':
                reviews.append(submission.selftext)
        except:
            pass  # Skip if language detection fails (e.g., very short text)

print('Collected:', len(reviews), 'English Inception-related reviews/posts.')


reviews[:2]


analyzer = SentimentIntensityAnalyzer()

for review in reviews[:5]:  # Show a few examples
    scores = analyzer.polarity_scores(review)
    print('Sentiment: ', scores)


analyzer = SentimentIntensityAnalyzer()

for i, review in enumerate(reviews[:5]):  # Show a few examples
    scores = analyzer.polarity_scores(review)
    print(f"Review {i+1} Sentiment: {scores}")





inception = pd.DataFrame({'reviews':reviews})
inception.head()


vader = SentimentIntensityAnalyzer()


vader


inception['vader_score'] = inception['reviews'].apply(lambda x: vader.polarity_scores(x))
# inception['vader_score']


inception['vader_score'][0]
# inception['vader_score'].keys()


#add vader scores into the Inception dataframe
inception['positive sentiment score'] = inception['reviews'].apply(lambda x: vader.polarity_scores(x)['pos'])
inception['negative sentiment score'] = inception['reviews'].apply(lambda x: vader.polarity_scores(x)['neg'])
inception['neutral sentiment score'] = inception['reviews'].apply(lambda x: vader.polarity_scores(x)['neu'])
inception['compound_score'] = inception['reviews'].apply(lambda x: analyzer.polarity_scores(x)['compound'])


# Apply sentiment analysis
# inception['compound_score'] = inception['reviews'].apply(lambda x: analyzer.polarity_scores(x)['compound'])

# Optional: classify sentiment as Positive / Negative / Neutral
def classify_sentiment(score):
    if score >= 0.05:
        return 'Positive'
    elif score <= -0.05:
        return 'Negative'
    else:
        return 'Neutral'

inception['sentiment'] = inception['compound_score'].apply(classify_sentiment)

inception[['reviews', 'compound_score', 'sentiment']].head()


inception.shape


inception.head()


inception.to_csv('processed_data/inception.csv')


#creating a stop words list - these words don't add much value
from nltk.corpus import stopwords
import string

#adding additional words I don't find to be that valuable
additional_stopwords = ['https', 'I', 'com', 'http', 'RT', 'co', 'the', 'amp', 'the', 'lt', 'A', 'www', 'rt', 'got', 'get', 
                        'ly', 'u', 'also', 'like', 'could', 'get', 'go', 'know']

stopwords_list = stopwords.words('english')

stopwords_list += list(string.punctuation)

stopwords_list += additional_stopwords

print(stopwords_list)


#isolating every word within tweets for the Inception movie (not including stop words)
# Split each review into words
inception["split_review"] = inception['reviews'].str.split()


# inception.shape
inception.head()


# inception = inception[~inception['split_review'].isin(stopwords_list)]


inception.shape


# Ensure all stopwords are lowercase for consistency
stopwords_list = set([word.lower() for word in stopwords_list])

# Tokenize and remove stopwords
inception["split_review"] = inception['reviews'].str.lower().str.split()
inception["filtered_review"] = inception["split_review"].apply(lambda tokens: [word for word in tokens if word not in stopwords_list])


all_words = inception["filtered_review"].explode()
top_words = Counter(all_words).most_common(20)
print(top_words)


inception.head(2)
all_words[:7]
# top_words
dict(top_words)


# Create frequency distribution
fdist = FreqDist(dict(top_words))

# Now you can inspect fdist
print(fdist.most_common(20))  # Top 10 most frequent words


#line plot
fig, ax = plt.subplots(1,1, figsize=(15,8)) 
ax = fdist.plot(25, title="Most Common Words about Inception")
ax.set_xlabel("Words")
ax.set_ylabel("Frequency")
plt.tight_layout()
plt.show()

plt.savefig('images/inceptionfrequency.jpg')


# Split words and counts for plotting
words, counts = zip(*top_words)

fig, ax = plt.subplots(figsize=(15, 8))
ax.bar(words, counts, color=cm.viridis_r(np.linspace(.4,.8, 30)))
ax.set_title("Most Common Words about Inception")
ax.set_xlabel("Words")
ax.set_ylabel("Frequency")
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

# plt.savefig('images/barInceptionFrequency.png')





top_words = Counter(all_words).most_common(95)
# Create frequency distribution from most common
fdist = FreqDist(dict(top_words))

# Create the word cloud:
wordcloud = WordCloud(colormap='twilight_shifted').generate_from_frequencies(fdist)

# Display the generated image w/ matplotlib:
plt.figure(figsize=(10,10), facecolor='k')
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.tight_layout(pad=0)

plt.show()

# Uncomment the next line if you want to save your image:
plt.savefig('images/inceptionwordcloud.jpg')


# Let's assume your DataFrame is named df and 'split_review' is already a list of words
# Flatten all lists from the split_review column
all_words = [word for sublist in inception['split_review'] for word in sublist]

# Convert stopwords to a set for faster lookup
stop_words = set(stopwords.words('english'))

# Remove stopwords and lowercase the words
filtered_words = [word.lower() for word in all_words if word.lower() not in stop_words]
filtered_words[:7]
# # Create frequency distribution
# fdist = FreqDist(filtered_words)

# # Now you can inspect fdist
# print(fdist.most_common(20))  # Top 10 most frequent words


# Step 1: Flatten the split_review column into one list of words with a DataFrame
words_df = inception['split_review'].explode().to_frame(name='word')

# Step 2: Lowercase words and filter out stopwords using `isin`
stop_words = set(stopwords.words('english'))
words_df['word'] = words_df['word'].str.lower()
filtered_df = words_df[~words_df['word'].isin(stop_words)]

# Step 3: Get word frequency using value_counts
fdist = filtered_df['word'].value_counts()

# Optional: Convert to dictionary or inspect top words
print(fdist.head(10))


# inception_words = inception['split_review'].explode().reset_index(drop=True)
# inception['split_review'].explode().reset_index()
# pd.DataFrame(inception['split_review'].explode())
# words_df.head()
# pd.DataFrame(inception['split_review'].explode().rename(columns={'split_review':'word'})


#freqDist - frequency of words within a text, in this case tweets about Inception
# fdist = FreqDist(inception['split_review'][0])


# fdist.most_common(2)


#distribution of vader score
# fig, ax = plt.subplots(figsize=(15,7))
# ax = sns.distplot(inception['vader_score'], color = 'firebrick')
# plt.xlabel('Sentiment Score', fontsize=18)
# plt.ylabel('Tweets per Score', fontsize=18)
# plt.title('VADER Sentiment of Inception', fontsize=22)
# plt.tick_params(labelsize='large')

# plt.savefig('images/VADERinception.jpg')





# Extract compound scores into a new column
inception['compound'] = inception['vader_score'].apply(lambda x: x['compound'] if isinstance(x, dict) else None)

# Plot the distribution
plt.figure(figsize=(15,7))
sns.histplot(inception['compound'].dropna(), color='firebrick', bins=30, kde=True)
plt.xlabel('Compound Sentiment Score', fontsize=18)
plt.ylabel('Number of Tweets', fontsize=18)
plt.title('Distribution of Compound Sentiment Scores', fontsize=20)
plt.show()

plt.savefig('Reddit_NLP_Sentiment/images/VADERinception.jpg')


inception.head(2)


#bar plot representation of negative, neutral, and positive vader scores

avg_scores = {
    'Negative': inception['negative sentiment score'].mean(),
    'Neutral': inception['neutral sentiment score'].mean(),
    'Positive': inception['positive sentiment score'].mean()
}

fig, ax = plt.subplots(figsize=(12,8))
ax.bar(avg_scores.keys(), avg_scores.values(), color =['red', 'yellow', 'green'])
plt.title('Inception VADER Sentiments', fontsize=25)
plt.xlabel('Sentiment Class', fontsize=20)
plt.ylabel('Tweets per Class', fontsize=20)
plt.tick_params(labelsize='large')

# plt.savefig('Reddit_NLP_Sentiment/images/inceptionsentiments.jpg')





#stemmed tweets
ps = PorterStemmer()


inception.head(2)
# all_words[:4]


top_words[:5]


# Apply stemming to each word in all_words
# inception["stemmed_review"] = inception["filtered_review"].apply(lambda tokens: [ps.stem(word) for word in tokens])

inception["stemmed_review"] = inception["filtered_review"].apply(
    lambda tokens: [ps.stem(re.sub(f"[{re.escape(string.punctuation)}]", "", word)) for word in tokens if re.sub(f"[{re.escape(string.punctuation)}]", "", word)]
)


inception.head(3)


# stemmed_words[:5]


# Remove punctuation using regex and stem
# all_words_cleaned = all_words.apply(lambda w: ps.stem(re.sub(f"[{re.escape(string.punctuation)}]", "", w)))
# Apply stemming to each word in all_words
# stemmed_words = all_words_cleaned.apply(ps.stem)


# stemmed_words[:5]


# stopwords_list #dict


# sparse Document Term Matrix

# vec = CountVectorizer(stop_words=list(stopwords_list))
# X = vec.fit_transform(stemmed_words.values.tolist())

# df = pd.DataFrame(X.toarray(), columns = vec.get_feature_names_out())
# print(df.shape)
# df.head()


# Join stemmed words per document into a single string
stemmed_words = inception["stemmed_review"].apply(lambda tokens: " ".join(tokens))

# Vectorize
vec = CountVectorizer(stop_words=list(stopwords_list))
X = vec.fit_transform(stemmed_words)

# Create DataFrame from the DTM
df = pd.DataFrame(X.toarray(), columns=vec.get_feature_names_out())

print(df.shape)
df.head()


#dictionary of numbers to words
dictionary = corpora.Dictionary(inception["stemmed_review"])
type(dictionary)
len(dictionary)


dictionary[1]



